import chromadb
import math
import numpy as np
from googlesearch import search
import requests
from bs4 import BeautifulSoup
from numpy import clip
from sentence_transformers import SentenceTransformer

# âœ… ëª¨ë¸ì„ ë²¡í„° ì €ì¥í•  ë•Œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")


# ğŸ”¹ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
chroma_client = chromadb.HttpClient(host="localhost", port=8000)
collection = chroma_client.get_or_create_collection(name="documents")

# âœ… 1. íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ RAG ì ìš© ì—¬ë¶€ íŒë‹¨ 
def should_apply_rag(query: str, top_k_final: int = 20, threshold: float = 0.7, scale: float = 30.0):
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embedding_model.encode(query).tolist()

    # ì»¬ë ‰ì…˜ì—ì„œ ì¿¼ë¦¬ ì‹¤í–‰
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=10
    )

    retrieved_docs = results.get("documents", [[]])[0]
    retrieved_scores = results.get("distances", [[]])[0]  # ê¸°ë³¸ì ìœ¼ë¡œ L2 distance ì œê³µë¨
    retrieved_metadata = results.get("metadatas", [[]])[0]

    all_docs = []
    print("\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ë° ìœ ì‚¬ë„ ì ìˆ˜:")

    for doc, score, meta in zip(retrieved_docs, retrieved_scores, retrieved_metadata):
        # scaling factor ì ìš© í›„ L2 distanceë¥¼ ì§€ìˆ˜ ê°ì‡  í•¨ìˆ˜ë¡œ similarityë¡œ ë³€í™˜
        similarity = math.exp(-score / scale)
        similarity = clip(similarity, 0, 1)  # ê°’ì´ 0~1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ë³´ì •

        print(f"ğŸ“„ ë¬¸ì„œ: {doc[:50]}... | ğŸ”¢ L2 distance: {score:.4f} | ğŸ”¥ ë³€í™˜ëœ ìœ ì‚¬ë„: {similarity:.4f}")

        if similarity < threshold:
            print(f"âš ï¸ ìœ ì‚¬ë„ê°€ {similarity:.4f}ë¡œ threshold({threshold})ë³´ë‹¤ ë‚®ì•„ í•„í„°ë§ë¨.")
            continue

        all_docs.append((doc, similarity, meta["filename"]))

    # ìƒìœ„ ìœ ì‚¬ë„ ë¬¸ì„œë§Œ ì„ íƒ
    all_docs = sorted(all_docs, key=lambda x: x[1], reverse=True)[:top_k_final]

    if not all_docs:
        print("âŒ ìœ ì‚¬í•œ ë¬¸ì„œ ì—†ìŒ â†’ RAG ë¯¸ì ìš©")
        return False, ""

    combined_context = "\n\n".join(doc for doc, _, _ in all_docs)
    print(f"\nâœ… RAG ì ìš©ë¨! (ì‚¬ìš©ëœ ë¬¸ì„œ ê°œìˆ˜: {len(all_docs)})")
    return True, combined_context 
 

# âœ… 2. Google ê²€ìƒ‰ ìˆ˜í–‰
def search_web(query: str, num_results=2):
    """Google Searchë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
    try:
        search_results = list(search(query, num_results=num_results, lang="ko"))
        return search_results
    except Exception as e:
        print(f"âŒ Error during Google Search: {e}")
        return []

# âœ… 3. ì›¹í˜ì´ì§€ì—ì„œ ì£¼ìš” í…ìŠ¤íŠ¸ í¬ë¡¤ë§
def extract_text_from_url(url: str):
    """ì›¹í˜ì´ì§€ì˜ ì£¼ìš” ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.text, "html.parser")

        paragraphs = soup.find_all("p")  # ì£¼ìš” ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
        extracted_text = " ".join([p.get_text() for p in paragraphs])

        return extracted_text[:1000]  # LLM ì…ë ¥ì„ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ ê¸¸ì´ ì œí•œ
    except Exception as e:
        print(f"âŒ Error extracting text from {url}: {e}")
        return None
