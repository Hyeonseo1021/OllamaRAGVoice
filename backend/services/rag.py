import chromadb
import math
import numpy as np
from googlesearch import search
import requests
from bs4 import BeautifulSoup
from numpy import clip
from sentence_transformers import SentenceTransformer

# âœ… ëª¨ë¸ì„ ë²¡í„° ì €ì¥í•  ë•Œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ”¹ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
chroma_client = chromadb.HttpClient(host="localhost", port=8000)
collection = chroma_client.get_or_create_collection(name="documents")

# âœ… 1. íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ RAG ì ìš© ì—¬ë¶€ íŒë‹¨ 
def should_apply_rag(query: str, top_k_final: int = 20, threshold: float = 0.7):
    """ íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ RAG ì ìš© ì—¬ë¶€ íŒë‹¨ (L2 Distance ì •ê·œí™” + ìœ ì‚¬ë„ ë³€í™˜ ê°œì„ ) """

    # âœ… L2 Distance ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™” ì—†ì´ ì„ë² ë”© ìƒì„±
    query_embedding = np.array(embedding_model.encode(query, normalize_embeddings=False))

    # âœ… ChromaDBì—ì„œ L2 Distance ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=50  # âœ… ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜ ì¦ê°€
    )

    retrieved_docs = results.get("documents", [[]])[0]
    retrieved_scores = results.get("distances", [[]])[0]  # âœ… L2 Distance ê°’ ê°€ì ¸ì˜¤ê¸°
    retrieved_metadata = results.get("metadatas", [[]])[0]

    if not retrieved_docs:
        print("âŒ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŒ â†’ RAG ë¯¸ì ìš©")
        return False, ""

    all_docs = []
    print("\nğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ë° L2 Distance ì ìˆ˜:")

    # âœ… L2 Distance ê°’ ì •ê·œí™”: ìµœì†Œ/ìµœëŒ€ ê±°ë¦¬ êµ¬í•´ì„œ ìŠ¤ì¼€ì¼ë§
    min_dist = min(retrieved_scores) if retrieved_scores else 0
    max_dist = max(retrieved_scores) if retrieved_scores else 1

    for doc, score, meta in zip(retrieved_docs, retrieved_scores, retrieved_metadata):
        # âœ… ìœ ì‚¬ë„ ë³€í™˜ ë°©ë²• ê°œì„ 
        similarity = 1 / (1 + score)  # âœ… L2 Distance ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ìœ ì‚¬ë„ ë°˜í™˜
        similarity = (max_dist - score) / (max_dist - min_dist + 1e-9)  # âœ… Min-Max ì •ê·œí™” ì¶”ê°€

        print(f"ğŸ“„ ë¬¸ì„œ: {doc[:50]}... | ğŸ”¢ L2 Distance: {score:.4f} | ğŸ”¥ ë³€í™˜ëœ ìœ ì‚¬ë„: {similarity:.4f}")

        all_docs.append((doc, similarity, meta["filename"]))

    # âœ… ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ í›„ top_k ê°œë§Œ ì„ íƒ
    all_docs = sorted(all_docs, key=lambda x: x[1], reverse=True)[:top_k_final]

    # âœ… ìƒìœ„ ë¬¸ì„œ ì¤‘ threshold ì´í•˜ì¸ ë¬¸ì„œë§Œ í•„í„°ë§
    filtered_docs = [(doc, sim, meta) for doc, sim, meta in all_docs if sim >= threshold]

    if not filtered_docs:
        print("âŒ ìœ ì‚¬í•œ ë¬¸ì„œ ì—†ìŒ â†’ RAG ë¯¸ì ìš©")
        return False, ""

    combined_context = "\n\n".join(doc for doc, _, _ in filtered_docs)
    print(f"\nâœ… RAG ì ìš©ë¨! (ì‚¬ìš©ëœ ë¬¸ì„œ ê°œìˆ˜: {len(filtered_docs)})")
    return True, combined_context
 

# âœ… 2. Google ê²€ìƒ‰ ìˆ˜í–‰
def search_web(query: str, num_results=2):
    """Google Searchë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
    try:
        search_results = list(search(query, num_results=num_results, lang="ko"))
        return search_results
    
    except Exception as e:
        print(f"âŒ Error during Google Search: {e}")
        return ["Error fetching search results."]

# âœ… 3. ì›¹í˜ì´ì§€ì—ì„œ ì£¼ìš” í…ìŠ¤íŠ¸ í¬ë¡¤ë§
def extract_text_from_url(url: str):
    """ì›¹í˜ì´ì§€ì˜ ì£¼ìš” ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.text, "html.parser")

        paragraphs = soup.find_all("p")  # ì£¼ìš” ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
        extracted_text = " ".join([p.get_text() for p in paragraphs])

        return extracted_text[:1000]  # LLM ì…ë ¥ì„ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ ê¸¸ì´ ì œí•œ\
    
    except Exception as e:
        print(f"âŒ Error extracting text from {url}: {e}")
        return None
