from googlesearch import search
import requests
from bs4 import BeautifulSoup

# ğŸ”¹ RAGê°€ ì ìš©ë  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
RAG_KEYWORDS = ["ìµœì‹ ", "ìµœê·¼", "ì—…ë°ì´íŠ¸", "ë‰´ìŠ¤", "ë…¼ë¬¸", "ì—°êµ¬", "ë°œí‘œ", "ìƒˆë¡œìš´", "ìŠ¤ë§ˆíŠ¸íŒœ"]

# âœ… 1. íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ RAG ì ìš© ì—¬ë¶€ íŒë‹¨
def should_apply_rag(query: str) -> bool:
    """ì§ˆë¬¸ì— íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ RAG ì ìš© ì—¬ë¶€ ê²°ì •"""
    return any(keyword in query.lower() for keyword in RAG_KEYWORDS)

# âœ… 2. Google ê²€ìƒ‰ ìˆ˜í–‰
def search_web(query: str, num_results=2):
    """Google Searchë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # `googlesearch.search`ë¥¼ ì‚¬ìš©í•´ Googleì—ì„œ ê²€ìƒ‰
        search_results = list(search(query, num_results=num_results, lang="ko"))
        return search_results
    except Exception as e:
        print(f"âŒ Error during Google Search: {e}")
        return []

# âœ… 3. ì›¹í˜ì´ì§€ì—ì„œ ì£¼ìš” í…ìŠ¤íŠ¸ í¬ë¡¤ë§
def extract_text_from_url(url: str):
    """ì›¹í˜ì´ì§€ì˜ ì£¼ìš” ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.text, "html.parser")

        paragraphs = soup.find_all("p")  # ì£¼ìš” ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
        extracted_text = " ".join([p.get_text() for p in paragraphs])

        return extracted_text[:1000]  # LLM ì…ë ¥ì„ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ ê¸¸ì´ ì œí•œ
    except Exception as e:
        print(f"âŒ Error extracting text from {url}: {e}")
        return None

# âœ… 4. ê²€ìƒ‰ & í¬ë¡¤ë§ í†µí•©
def retrieve_relevant_text(query: str):
    """ê²€ìƒ‰ ë° í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ì—¬ LLMì— ì œê³µí•  ë¬¸ë§¥ì„ êµ¬ì„±"""
    print("ğŸ” RAG ì ìš©: Google ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
    urls = search_web(query)
    if not urls:
        print("ğŸ” No URLs found for query.")
        return "í˜„ì¬ Google ê²€ìƒ‰ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."

    # ê° URLë³„ë¡œ í¬ë¡¤ë§ëœ ê²°ê³¼ ì¶œë ¥
    extracted_texts = []
    for url in urls:
        print(f"ğŸ”— URL Found: {url}")
        text = extract_text_from_url(url)
        if text:
            print(f"âœ… Extracted Text from {url}:\n{text[:200]}...\n")  # ì•ë¶€ë¶„ 200ìë§Œ ì¶œë ¥
            extracted_texts.append(text)
        else:
            print(f"âŒ Failed to extract text from {url}.\n")
    
    # None ê°’ ì œê±° í›„ í•˜ë‚˜ì˜ ë¬¸ë§¥ìœ¼ë¡œ ê²°í•©
    context = "\n".join(extracted_texts)
    return context if context else "No relevant information found."
