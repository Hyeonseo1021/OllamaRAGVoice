import pandas as pd
import json
from fastapi import UploadFile, HTTPException
from .extraction import extract_text, calculate_file_hash
from langchain.text_splitter import CharacterTextSplitter
from sentence_transformers import SentenceTransformer

uploaded_hashes = set()  # í•´ì‹œ ì €ì¥ì†Œ
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # âœ… ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ

async def process_uploaded_file(file: UploadFile, collection_documents, collection_data_files):
    """ğŸ“‚ íŒŒì¼ ì—…ë¡œë“œ í›„ ì„ë² ë”© ìƒì„± ë° ChromaDB ì €ì¥ (CSV/JSON ë¶„ë¦¬)"""
    original_filename = file.filename
    file_content = await file.read()

    # ğŸ”¹ íŒŒì¼ í•´ì‹œê°’ ê³„ì‚°
    file_hash = calculate_file_hash(file_content)

    # ğŸ”¹ ì¤‘ë³µ ì—…ë¡œë“œ ë°©ì§€
    if file_hash in uploaded_hashes:
        raise HTTPException(status_code=400, detail="âš ï¸ ì´ë¯¸ ì—…ë¡œë“œëœ íŒŒì¼ì…ë‹ˆë‹¤.")

    uploaded_hashes.add(file_hash)  # í•´ì‹œ ì €ì¥

    # ğŸ”¹ íŒŒì¼ í™•ì¥ì í™•ì¸
    file_ext = original_filename.split(".")[-1].lower()
    
    if file_ext in ["csv", "json", "xlsx"]:
        docs = process_data_file(file_content, file_ext)
        collection = collection_data_files  # âœ… CSV/JSONì€ ë³„ë„ ì»¬ë ‰ì…˜ì— ì €ì¥
    else:
        pages = extract_text(original_filename, file_content)
        if not pages or (isinstance(pages, list) and not any(pages)):
            return {"error": f"âŒ {original_filename}ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # ğŸ”¹ 1000ì ë‹¨ìœ„ë¡œ ë¶„í• 
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = [chunk for page in pages for chunk in text_splitter.split_text(page)]
        collection = collection_documents  # âœ… ì¼ë°˜ ë¬¸ì„œëŠ” ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©

    print(f"ğŸ“‚ {original_filename}ì—ì„œ {len(docs)}ê°œì˜ ë¬¸ì„œ ì¡°ê° ìƒì„±ë¨.")

    # âœ… ì„ë² ë”© ë° ChromaDB ì €ì¥
    embedding_dim = embedding_model.get_sentence_embedding_dimension()

    for i, doc in enumerate(docs):
        try:
            vector = embedding_model.encode(doc).tolist()
            if len(vector) != embedding_dim:
                print(f"âŒ {original_filename}-{i} ë²¡í„° í¬ê¸° ì˜¤ë¥˜! ì˜ˆìƒ {embedding_dim}, ì‹¤ì œ {len(vector)}")
                continue
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            continue

        collection.add(
            ids=[f"{original_filename}-{i}"],
            embeddings=[vector],
            metadatas=[{"filename": original_filename, "hash": file_hash, "text": doc}],
            documents=[doc]
        )

    print(f"âœ… {original_filename}ì´(ê°€) ChromaDBì— ì €ì¥ë¨. (ì´ {len(docs)}ê°œ)")

    return {"message": f"âœ… {original_filename} ì—…ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ!"}



def process_data_file(file_content: bytes, file_ext: str):
    """ğŸ“Š CSV/JSON íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ChromaDBì— ì €ì¥í•  ë¬¸ì„œ ìƒì„±"""
    docs = []
    
    if file_ext == "csv":
        df = pd.read_csv(pd.io.common.BytesIO(file_content))
    elif file_ext == "xlsx":
        df = pd.read_excel(pd.io.BytesIO(file_content))
    elif file_ext == "json":
        data = json.loads(file_content.decode("utf-8"))
        df = pd.DataFrame(data)

    # âœ… ë°ì´í„° ë³€í™˜ (ì»¬ëŸ¼ëª…ì„ í¬í•¨í•˜ì—¬ ìì—°ì–´ë¡œ ë³€í™˜)
    for _, row in df.iterrows():
        doc_text = ", ".join([f"{col}: {row[col]}" for col in df.columns if pd.notna(row[col])])
        docs.append(doc_text)

    return docs
