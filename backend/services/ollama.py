import ollama
from services.rag import should_apply_rag, search_web
from services.manage_data import search_growth_data_in_chromadb, filter_growth_data

# âœ… GPT ê¸°ë°˜ ì§ˆë¬¸ ë¶„ì„ í•¨ìˆ˜
def classify_question_type(prompt: str) -> str:
    """
    ğŸ“Œ GPT-4 (ë˜ëŠ” LLaMA) ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  
    'DATA', 'RAG', 'BOTH' ì¤‘ í•˜ë‚˜ë¥¼ ë°˜í™˜
    """
    system_prompt = """
    ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸íŒœ ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” AIì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì‹­ì‹œì˜¤:
    
    - DATA: í˜„ì¬ ì˜¨ë„, ìŠµë„, ì„¼ì„œ ë°ì´í„° ë“± **ì‹¤ì‹œê°„ ì •ë³´**ì™€ íŠ¹ì • ë‚ ì§œ, ë†ê°€ëª…, ì„¼ì„œ ë°ì´í„°, ë”¸ê¸° íŠ¹ì„± ë“± **ì •í™•í•œ ê°’**ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°
    - RAG: ìµœì  ì˜¨ë„, ì‘ë¬¼ ê´€ë¦¬ë²•, ìŠ¤ë§ˆíŠ¸íŒœ ìš´ì˜ ì§€ì‹ ë“± **ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´**ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°
    - BOTH: ì‹¤ì‹œê°„ ë°ì´í„°ì™€ ë¬¸ì„œ ì •ë³´ê°€ **ë‘˜ ë‹¤ í•„ìš”í•œ ê²½ìš°** (ì˜ˆ: "ì§€ê¸ˆ ì˜¨ë„ê°€ ì ë‹¹í•´?")
    
    ì˜ˆì‹œ:
    1. "í˜„ì¬ ì˜¨ë„ ëª‡ ë„ì•¼?" â†’ DATA
    2. "ë”¸ê¸° ìµœì  ì˜¨ë„ëŠ” ì–¼ë§ˆì•¼?" â†’ RAG
    3. "ì§€ê¸ˆ ì˜¨ë„ê°€ ìµœì  ë²”ìœ„ì•¼?" â†’ BOTH
    4. "ì§€ë‚œì£¼ ë†ê°€ë³„ í‰ê·  ìŠµë„ëŠ”?" -> DATA
    5. "ë”¸ê¸° ë³‘í•´ì¶© ê´€ë¦¬ ë°©ë²•ì€? -> RAG
    6. "í˜„ì¬ ì¡°ë„ì™€ ìƒìœ¡ ë‹¨ê³„ë³„ ìµœì  ì¡°ë„ ë¹„êµí•´ì¤˜." -> BOTH

    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ìœ„ ê·œì¹™ì— ë”°ë¼ ë¶„ë¥˜í•˜ì„¸ìš”. **ì‘ë‹µì€ ë°˜ë“œì‹œ 'DATA', 'RAG', 'BOTH' ì¤‘ í•˜ë‚˜ë¡œë§Œ í•˜ì‹­ì‹œì˜¤.**
    """

    response = ollama.chat(model="llama3", messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ])

    # âœ… "DATA", "RAG", "BOTH" ì¤‘ í•˜ë‚˜ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •
    if hasattr(response, "message") and hasattr(response.message, "content"):
        response_text = response.message.content.strip().upper()
        if response_text in ["DATA", "RAG", "BOTH"]:
            print(response_text)
            return response_text

    return "UNKNOWN"  # ì˜ˆì™¸ ì²˜ë¦¬

async def query_olama(prompt: str, use_rag: bool = False, threshold: float = 0.7) -> str:

    data_source = classify_question_type(prompt)
    print(f"ğŸ” ë°ì´í„° ë°©ì‹ ê²°ì •: {data_source}")

    context = ""

        # âœ… DATA ê²€ìƒ‰ (ì‹¤ì‹œê°„ ë°ì´í„° í•„ìš”)
    if data_source in ["DATA", "BOTH"]:
        raw_data = search_growth_data_in_chromadb(prompt, data_source)  # ğŸ”¹ ìµœì‹  ì„¼ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        data_context = filter_growth_data(raw_data, prompt)
        
        if data_context:
            context += f"\n\n### ìƒìœ¡ ë°ì´í„°:\n{data_context}"
        
    # âœ… RAG ë¬¸ì„œì—ì„œ ê²€ìƒ‰ (ì§€ì‹ ê¸°ë°˜ ì •ë³´ í•„ìš”)
    if data_source in ["RAG", "BOTH"]:
        if use_rag:
            use_rag, rag_context = should_apply_rag(prompt, top_k_final=20, threshold=threshold)

            if use_rag and rag_context:
                print("âœ… RAG ì ìš©: ChromaDB ê²€ìƒ‰ ê²°ê³¼ í™œìš©")
                print(f"ğŸ” ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©: {context[:500]}...")  # âœ… ì»¨í…ìŠ¤íŠ¸ í™•ì¸ìš© ì¶œë ¥
                context += f"\n\n### RAG ë¬¸ì„œ ë°ì´í„°:\n{rag_context}"


            else:
                print("âŒ ë¬¸ì„œ ìœ ì‚¬ë„ ë‚®ìŒ â†’ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰")
                web_context = search_web(prompt) or "No relevant documents found."
                context += f"\n\n### ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n{web_context}"          
    
    if context:
        query_text = f"""
        í•­ìƒ ê³µì‹ì ì¸ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.
        ì œê³µëœ ë¬¸ë§¥(context) ê¸°ë°˜ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤. ë¬¸ë§¥ì— í¬í•¨ë˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
        ***ë¬¸ë§¥ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ì œê³µë˜ì§€ ì•Šì€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.***
        ë¬¸ë§¥ì— ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•˜ì‹­ì‹œì˜¤:
        ***ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.***

        {context}
        ### ì‚¬ìš©ì ì§ˆë¬¸
        {prompt}
        ### AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µ (in Korean):
        """

    else:
        print("ğŸ’¡ RAG ë²„íŠ¼ OFF ë˜ëŠ” ë°ì´í„° ì—†ìŒ â†’ ì¼ë°˜ ëª¨ë¸ ì‘ë‹µ")
        query_text = prompt  # ê·¸ëƒ¥ Olama ëª¨ë¸ ì‚¬ìš©

    # ğŸ§  Olama ëª¨ë¸ í˜¸ì¶œ
    response = ollama.chat(model="gemma:7b", messages=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì¤‘í•œ í•œêµ­ì–´  AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ê³µì‹ì ì¸ í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤"},
        {"role": "user", "content": query_text}
    ])

    # âœ… ì‘ë‹µì´ `Message` ê°ì²´ì´ë¯€ë¡œ `.message.content` ì‚¬ìš©
    if hasattr(response, "message") and hasattr(response.message, "content"):
        return response.message.content

    return "Error: Could not retrieve response from Olama"
