import ollama
from services.rag import should_apply_rag, search_web
from services.manage_data import search_growth_data_in_chromadb, filter_growth_data

# âœ… GPT ê¸°ë°˜ ì§ˆë¬¸ ë¶„ì„ í•¨ìˆ˜
def classify_question_type(prompt: str) -> str:
    
    system_prompt = """
    ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸íŒœ ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” AIì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ **ë°˜ë“œì‹œ** 'DATA', 'RAG', 'BOTH' ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
    
    ** ë¶„ë¥˜ ê¸°ì¤€ **
    - DATA: í˜„ì¬ ì˜¨ë„, ìŠµë„, ì„¼ì„œ ë°ì´í„° ë“± **ì‹¤ì‹œê°„ ì •ë³´**ì™€ íŠ¹ì • ë‚ ì§œ, ë†ê°€ëª…, ì„¼ì„œ ë°ì´í„°, ë”¸ê¸° íŠ¹ì„± ë“± **ì •í™•í•œ ê°’**ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°
    - RAG: ìµœì  ì˜¨ë„, ìƒìœ¡ ì¡°ê±´, ì‘ë¬¼ ê´€ë¦¬ë²•, ìŠ¤ë§ˆíŠ¸íŒœ ìš´ì˜ ì§€ì‹ ë“± **ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´**ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°
    - BOTH: í˜„ì¬ ë°ì´í„°ë¥¼ **ê¸°ì¤€ ì •ë³´ì™€ ë¹„êµì—ì•¼ í•˜ëŠ” ê²½ìš°** (ì˜ˆ: "ì§€ê¸ˆ ì˜¨ë„ê°€ ì ë‹¹í•´?", "ì§€ê¸ˆ CO2 ë†ë„ëŠ” ê¸°ì¤€ì— ë§ì•„?")
    
    ì˜ˆì‹œ:
    1. "í˜„ì¬ ì˜¨ë„ ëª‡ ë„ì•¼?" â†’ DATA
    2. "ë”¸ê¸° ìµœì  ì˜¨ë„ëŠ” ì–¼ë§ˆì•¼?" â†’ RAG
    3. "ì§€ê¸ˆ ì˜¨ë„ê°€ ìµœì  ë²”ìœ„ì•¼?" â†’ BOTH
    4. "ì§€ë‚œì£¼ ë†ê°€ë³„ í‰ê·  ìŠµë„ëŠ”?" -> DATA
    5. "ë”¸ê¸° ë³‘í•´ì¶© ê´€ë¦¬ ë°©ë²•ì€? -> RAG
    6. "í˜„ì¬ ì¡°ë„ì™€ ìƒìœ¡ ë‹¨ê³„ë³„ ìµœì  ì¡°ë„ ë¹„êµí•´ì¤˜." -> BOTH

    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ìœ„ ê·œì¹™ì— ë”°ë¼ ë¶„ë¥˜í•˜ì„¸ìš”. **ì‘ë‹µì€ ë°˜ë“œì‹œ 'DATA', 'RAG', 'BOTH' ì¤‘ í•˜ë‚˜ë¡œë§Œ í•˜ì‹­ì‹œì˜¤.**
    """

    response = ollama.chat(model="llama3", messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ])

    print(f"ì›ë³¸ ì‘ë‹µ: {response}")
    # âœ… "DATA", "RAG", "BOTH" ì¤‘ í•˜ë‚˜ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •
    if hasattr(response, "message") and hasattr(response.message, "content"):
        response_text = response.message.content.strip().upper()
        
        # âœ… "DATA", "RAG", "BOTH"ê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ë°˜í™˜
        for category in ["DATA", "RAG", "BOTH"]:
            if category in response_text:
                print(f"ğŸ” ì§ˆë¬¸ ìœ í˜• ë¶„ì„ ê²°ê³¼: {category}")
                return category

    return "UNKNOWN"  # ì˜ˆì™¸ ì²˜ë¦¬

async def query_olama(prompt: str, threshold: float = 0.5) -> str:

    data_source = classify_question_type(prompt)
    print(f"ğŸ” ë°ì´í„° ë°©ì‹ ê²°ì •: {data_source}")

    context = ""

        # âœ… DATA ê²€ìƒ‰ (ì‹¤ì‹œê°„ ë°ì´í„° í•„ìš”)
    if data_source in ["DATA", "BOTH"]:
        raw_data = search_growth_data_in_chromadb(prompt)  # ğŸ”¹ ìµœì‹  ì„¼ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        
        if raw_data:
            context += f"\n\n### ìƒìœ¡ ë°ì´í„°:\n{raw_data}"
        
    # âœ… RAG ë¬¸ì„œì—ì„œ ê²€ìƒ‰ (ì§€ì‹ ê¸°ë°˜ ì •ë³´ í•„ìš”)
    if data_source in ["RAG", "BOTH"]:
       
        rag_context = should_apply_rag(prompt, top_k_final=20, threshold=threshold)

        if rag_context:
            print("âœ… RAG ì ìš©: ChromaDB ê²€ìƒ‰ ê²°ê³¼ í™œìš©")
            print(f"ğŸ” ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©: {context[:500]}...")  # âœ… ì»¨í…ìŠ¤íŠ¸ í™•ì¸ìš© ì¶œë ¥
            context += f"\n\n### RAG ë¬¸ì„œ ë°ì´í„°:\n{rag_context}"
        else:
            print("âŒ ë¬¸ì„œ ìœ ì‚¬ë„ ë‚®ìŒ â†’ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰")
            web_context = search_web(prompt) or "No relevant documents found."
            context += f"\n\n### ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n{web_context}"          
    
    if context:
        print(f"ìµœì¢… ì»¨í…ìŠ¤íŠ¸: {context[:500]}")
        query_text = f"""
        í•­ìƒ ê³µì‹ì ì¸ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.
        ì œê³µëœ ë¬¸ë§¥(context) ê¸°ë°˜ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤. ë¬¸ë§¥ì— í¬í•¨ë˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
        ***ë¬¸ë§¥ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ì œê³µë˜ì§€ ì•Šì€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.***
        ë¬¸ë§¥ì— ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•˜ì‹­ì‹œì˜¤:
        ***ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.***

        {context}
        ### ì‚¬ìš©ì ì§ˆë¬¸
        {prompt}
        ### AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µ (in Korean):
        """

    else:
        print("ğŸ’¡ ë°ì´í„° ì—†ìŒ â†’ ì¼ë°˜ ëª¨ë¸ ì‘ë‹µ")
        query_text = prompt  # ê·¸ëƒ¥ Olama ëª¨ë¸ ì‚¬ìš©

    # ğŸ§  Olama ëª¨ë¸ í˜¸ì¶œ
    response = ollama.chat(model="gemma:7b", messages=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì¤‘í•œ í•œêµ­ì–´  AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ê³µì‹ì ì¸ í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤"},
        {"role": "user", "content": query_text}
    ])

    # âœ… ì‘ë‹µì´ `Message` ê°ì²´ì´ë¯€ë¡œ `.message.content` ì‚¬ìš©
    if hasattr(response, "message") and hasattr(response.message, "content"):
        return response.message.content

    return "Error: Could not retrieve response from Olama"
