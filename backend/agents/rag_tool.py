import chromadb
import numpy as np
import re
from googlesearch import search
import requests
from bs4 import BeautifulSoup
from langchain.embeddings import HuggingFaceEmbeddings
from sklearn.metrics.pairwise import cosine_similarity
from langchain.tools import Tool

# âœ… 1ï¸âƒ£ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
chroma_client = chromadb.HttpClient(host="localhost", port=8000)
collection = chroma_client.get_or_create_collection(name="documents")

# âœ… 2ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ (ë¬¸ì„œ ê²€ìƒ‰ìš©)
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    encode_kwargs={"normalize_embeddings": True}
)

# âœ… 3ï¸âƒ£ ChromaDBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
def search_rag_data(query: str, top_k_final: int = 20, threshold: float = 0.5, min_docs: int = 3):
    """ ğŸ” ê°œì„ ëœ RAG ë¬¸ì„œ ê²€ìƒ‰: cosine ìœ ì‚¬ë„ ê¸°ë°˜ + ë³´ì¡° í•„í„° + ìµœì†Œ í™•ë³´ """

    # âœ… 1. cosine similarity ê¸°ë°˜ ì„ë² ë”© ìƒì„±
    query_embedding = embedding_model.embed_query(query)  # ì´ë¯¸ ì •ê·œí™”ë¨
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k_final * 2,  # í›„ë³´ ë„‰ë„‰íˆ í™•ë³´
        include=["documents", "metadatas", "distances"]
    )

    retrieved_docs = results.get("documents", [[]])[0]
    retrieved_scores = results.get("distances", [[]])[0]
    retrieved_metadata = results.get("metadatas", [[]])[0]

    if not retrieved_docs:
        print("âŒ No documents retrieved.")
        return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # âœ… 2. cosine ìœ ì‚¬ë„ ê³„ì‚° (scoreëŠ” distanceê°€ ì•„ë‹ˆë¼ similarity)
    all_docs = []
    for doc, dist, meta in zip(retrieved_docs, retrieved_scores, retrieved_metadata):
        similarity = 1 - dist  # cosine distance â†’ similarity
        all_docs.append((doc, similarity, meta["filename"]))

    # âœ… 3. threshold í•„í„° ë° ìœ ì‚¬ë„ ì •ë ¬
    sorted_docs = sorted(all_docs, key=lambda x: x[1], reverse=True)
    filtered_docs = [(doc, sim, meta) for doc, sim, meta in sorted_docs if sim >= threshold]

    # âœ… 4. í‚¤ì›Œë“œ ë³´ì¡° í•„í„° (ë³´ì™„ìš©)
    keywords = [kw for kw in re.findall(r"\b\w{2,}\b", query)]
    seen_meta = set(meta for _, _, meta in filtered_docs)

    for doc, sim, meta in sorted_docs:
        if len(filtered_docs) >= top_k_final:
            break
        if meta not in seen_meta and any(kw.lower() in doc.lower() for kw in keywords):
            print(f"ğŸ“Œ í‚¤ì›Œë“œ ê¸°ë°˜ ë³´ì¡° í¬í•¨: {meta}")
            filtered_docs.append((doc, sim, meta))
            seen_meta.add(meta)

    # âœ… 5. ìµœì†Œ í™•ë³´ ë³´ì¥
    for doc, sim, meta in sorted_docs:
        if len(filtered_docs) >= min_docs:
            break
        if meta not in seen_meta:
            filtered_docs.append((doc, sim, meta))
            seen_meta.add(meta)

    if not filtered_docs:
        return search_web(query)

    # âœ… 6. ì¶œë ¥ ì •ë¦¬
    formatted_docs = "\n\n".join([f"ğŸ“„ ë¬¸ì„œ: {meta}\n{doc}" for doc, _, meta in filtered_docs])
    return f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ë°ì´í„°:\n{formatted_docs}"

# âœ… 4ï¸âƒ£ Google ê²€ìƒ‰ ì‹¤í–‰
def search_web(query: str, num_results=2):
    """ Google ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ê´€ë ¨ ì›¹ í˜ì´ì§€ ë§í¬ ê°€ì ¸ì˜¤ê¸° """
    try:
        search_results = list(search(query, num_results=num_results, lang="ko"))
        if search_results:
            return extract_text_from_url(search_results[0])  # âœ… ì²« ë²ˆì§¸ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ í›„ ë°˜í™˜
        return "âŒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"âŒ Google ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# âœ… 5ï¸âƒ£ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ê¸°ëŠ¥
def extract_text_from_url(url: str):
    """ ì›¹í˜ì´ì§€ ì£¼ìš” í…ìŠ¤íŠ¸ í¬ë¡¤ë§ """
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        extracted_text = " ".join([p.get_text() for p in paragraphs])
        return f"ğŸŒ [ì¶œì²˜: {url}]\n" + extracted_text[:5000]  
    except Exception as e:
        return f"âŒ {url} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# âœ… 6ï¸âƒ£ LangChain ê¸°ë°˜ RAG Agent ì •ì˜
rag_tool = Tool(
    name="SmartFarmRAG",
    func=search_rag_data,
    description="ì‘ë¬¼ í’ˆì¢…, ë³‘í•´ì¶©, ì¬ë°° ë°©ë²• ë“± ë†ì—… ì§€ì‹ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
)