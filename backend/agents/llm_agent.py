import sys
import ollama
from langchain.agents import initialize_agent, AgentType
from langchain.tools import Tool
from langchain_ollama import OllamaLLM  # âœ… LangChain Ollama ì§€ì› LLM ì¶”ê°€
from langchain.prompts import PromptTemplate
from core.classifier import classify_question_type
from agents.data_tool import data_agent
from agents.rag_tool import rag_agent
from agents.both_tool import both_agent

# âœ… 1ï¸âƒ£ Ollama LangChain LLM ì„¤ì • (stop_sequences ì¶”ê°€)
llm = OllamaLLM(
    model="llama3",
)

# âœ… 2ï¸âƒ£ LangChain Tool ì„¤ì •
tools = [data_agent, rag_agent, both_agent]

# âœ… 3ï¸âƒ£ LangChainì—ì„œ ì‚¬ìš©í•  PromptTemplate ìƒì„± (í•„ìˆ˜ ë³€ìˆ˜ í¬í•¨)
prompt_template = PromptTemplate(
    input_variables=["prompt", "agent_scratchpad", "intermediate_steps", "context"],
    template="""
    Analyze the user's question and call the appropriate tools.

    ### User Input:
    {prompt}

    ### Provided Data:
    {context}

    ### Internal Scratchpad:
    {agent_scratchpad}

    ### Intermediate Steps:
    {intermediate_steps}

    ### Tool Selection Instructions:
    - Use **SmartFarmDataAgent** ONLY for sensor-related queries such as:
      - temperature, humidity, COâ‚‚ levels, light intensity, nutrient concentration, real-time readings

    - Use **SmartFarmRAGAgent** for knowledge-based queries such as:
      - crop varieties (e.g., ë”¸ê¸° í’ˆì¢…), cultivation methods, pest and disease information, general farming techniques, definitions, terminology

    - Use **SmartFarmBOTHAgent** if the question asks to compare:
      - real-time data with standards or recommendations in documents
      - sensor data vs document data

    ### Execution Instructions:
    - **After "Thought:", you must include "Action:".**
    - **Every Action must include 'Action Input:', even if no action is required.**
    - **If no additional action is needed, use: "Action: Final Answer\nAction Input: [your final response here]".**
    - **If an action is needed, specify the appropriate tool and its input.**
    - **If the question type is 'BOTH', always use 'both_agent' instead of calling 'data_agent' and 'rag_agent' separately.**
    - **After executing the Agent, combine the results and generate the final answer.**

    - **Analyze the retrieved data and generate the most appropriate response.**
    - **Use the data from 'Provided Data' to construct an accurate and well-formed answer.**
    - **Ensure the response is based solely on the given data and does not contain inferred or additional information.**
    - **The final response should be in natural Korean.**

    **ğŸ“¢ AI's Response:**  
    - **If you have the final answer, output it using:**  
      "Final Answer: [your response here]"
    - **If more action is needed, use "Action:" followed by "Action Input:"**
    """
)

# âœ… 4ï¸âƒ£ LangChain ìµœì‹  ë°©ì‹ìœ¼ë¡œ Agent ìƒì„± (ìµœëŒ€ 3íšŒ ë°˜ë³µ ì œí•œ)
print("ğŸš€ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘...")

try:
    agent_executor = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=3,  
        allow_ask_for_clarification=False  # âœ… ì¶”ê°€ ì§ˆë¬¸ ì—†ì´ ë°”ë¡œ ì¤‘ë‹¨
    )
    print("âœ… ì—ì´ì „íŠ¸ê°€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
     
except Exception as e:
    print(f"âŒ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
     

# âœ… 5ï¸âƒ£ Agentë¥¼ í™œìš©í•œ ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜
async def query_olama(prompt: str) -> str:
    print("ğŸš€ query_olama í•¨ìˆ˜ ì‹¤í–‰ë¨")
    print(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ : {prompt}")
     
    data_source = classify_question_type(prompt)
    print(f"ğŸ” ë°ì´í„° ë°©ì‹ ê²°ì •: {data_source}")

    context = ""
     
    try:
        if data_source == "UNKNOWN":
            context = prompt

        else:
            print("âš¡ LangChain Agent ì‹¤í–‰ ì¤‘...")
            result = agent_executor.invoke({"input": prompt}) # âœ… `invoke()` ì‚¬ìš©
           
            if isinstance(result, dict) and "output" in result:
                context = result["output"]
            else:
                context = str(result)
            print(f"âœ…ê²°ê³¼: {context[:200]}")  # ë¡œê·¸ ì¶œë ¥ ì‹œ ê¸¸ì´ ì œí•œ
             

    except Exception as e:
        print(f"âŒ Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
         
        return f"Error: {e}"

    if context:
        print(f"ğŸ” ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í™•ì¸: {context[:200]}")
         

        query_text = f"""
        í•­ìƒ ê³µì‹ì ì¸ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.
        ì œê³µëœ ë¬¸ë§¥(context) ê¸°ë°˜ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤. ë¬¸ë§¥ì— í¬í•¨ë˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
        ***ë¬¸ë§¥ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ì œê³µë˜ì§€ ì•Šì€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.***
        ë¬¸ë§¥ì— ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•˜ì‹­ì‹œì˜¤:
        ***ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.***

        {context}
        ### ì‚¬ìš©ì ì§ˆë¬¸
        {prompt}
        ### AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µ (in Korean):
        """
    else:
        print("ğŸ’¡ ë°ì´í„° ì—†ìŒ â†’ ì¼ë°˜ ëª¨ë¸ ì‘ë‹µ")
         
        query_text = prompt  # ê·¸ëƒ¥ Ollama ëª¨ë¸ ì‚¬ìš©

    # ğŸ§  Ollama ëª¨ë¸ í˜¸ì¶œ
    print("ğŸ§  Ollama ëª¨ë¸ í˜¸ì¶œ ì¤‘...")
     
    try:
        response = ollama.chat(model="gemma:7b", messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ì¤‘í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ê³µì‹ì ì¸ í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤"},
            {"role": "user", "content": query_text}
        ])
        print(f"âœ… Ollama ì‘ë‹µ ì™„ë£Œ: {response}")  # ë¡œê·¸ ì¶œë ¥
         
    except Exception as e:
        print(f"âŒ Ollama í˜¸ì¶œ ì˜¤ë¥˜: {e}")
         
        return f"Error: {e}"

    if hasattr(response, "message") and hasattr(response.message, "content"):
        return response.message.content

    return "Error: Could not retrieve response from Ollama"
